{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T15:19:14.045658Z",
     "start_time": "2024-04-23T15:19:14.034157Z"
    }
   },
   "source": [
    "import urllib\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a9419a811199b8ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T15:02:12.635149Z",
     "start_time": "2024-04-23T15:02:12.220537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get model\n",
    "!curl -o C:\\Users\\Nick Büttner\\PycharmProjects\\gesture_glide\\gesture_recognizer.task https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\n"
   ],
   "id": "a54d443fc946e2f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0000PK\u0003\u0004\u0014\u0000\u0000\u0000\u0000\u0000a\u001BšV·\u0003ÞçaOw\u0000aOw\u0000\u0014\u0000\u0000\u0000hand_landmarker.task\u0000\u0000PK\u0003\u0004\u0014\u0000\u0000\u0000\u0000\u0000a\u001BšVOà\u0007ï&´#\u0000&´#\u0000\u0014\u0000\u0000\u0000hand_detector.tflite\u001C\u0000\u0000\u0000TFL3\u0000\u0000\u0012\u0000 \u0000\u001C\u0000\u0018\u0000\u0014\u0000\u0010\u0000\f\u0000\u0004\u0000\u0012\u0000\u0000\u0000\u001C\u0000\u0000\u0000 \u0000\u0000\u0000¨\u0000\u0000\u0000¬¤\"\u0000¼¤\"\u0000¸²#\u0000\u0003\u0000\u0000\u0000\u0003\u0000\u0000\u0000`\u0000\u0000\u0000(\u0000\u0000\u0000\u0004\u0000\u0000\u0000´ÿÿÿ\u001D\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u000F\u0000\u0000\u0000TFLITE_METADATA\u0000Ôÿÿÿ\u001A\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0019\u0000\u0000\u0000reduced_precision_support\u0000\u0000\u0000\f\u0000\u0004\u0000\u0000\u0000\u0019\u0001\u0000\u0000\u0004\u0000\u0000\u0000\u0013\u0000\u0000\u0000min_runtime_version\u0000\u0002\u0000\u0000\u0000\u001B\u0001\u0000\u0000\u001C\u0001\u0000\u0000\u001E\u0001\u0000\u0000\u0000¤\"\u0000ø£\"\u0000è¡\"\u0000Ø \"\u0000À \"\u0000¨ \"\u0000Œ \"\u0000p \"\u0000@ \"\u0000\u0010 \"\u0000àŸ\"\u0000�Ÿ\"\u0000ÀŒ\"\u0000p†\"\u0000`~\"\u0000\u0010x\"\u0000\u0000p\"\u0000°i\"\u0000 a\"\u0000P[\"\u0000@S\"\u0000ðL\"\u0000`L\"\u0000P<\"\u0000À/\"\u0000°\u000F\"\u0000 \u0003\"\u0000\u0010ã!\u0000€Ö!\u0000p¶!\u0000à©!\u0000Ð‰!\u0000@}!\u00000|!\u0000 <!\u0000\u0010#!\u0000\u0000£ \u0000ð‰ \u0000à\t \u0000Ðð\u001F\u0000Àp\u001F\u0000°W\u001F\u0000 ×\u001E\u0000�¾\u001E\u0000€¼\u001E\u0000p¼\u001D\u0000`Š\u001D\u0000PŠ\u001B\u0000@X\u001B\u00000X\u0019\u0000 &\u0019\u0000\u0010&\u0017\u0000\u0000ô\u0016\u0000ðó\u0014\u0000àÁ\u0014\u0000ÐÁ\u0012\u0000À�\u0012\u0000°�\u0010\u0000 ]\u0010\u0000�]\u000E\u0000€+\u000E\u0000p+\f\u0000`ù\u000B\u0000Pù\t\u0000@ù\u0007\u00000Ç\u0007\u0000 Ç\u0005\u0000\u0010•\u0005\u0000\u0000•\u0003\u0000ðˆ\u0003\u0000à°\u0002\u0000Ð°\u0001\u0000À—\u0001\u0000°\u0017\u0001\u0000 þ\u0000\u0000�~\u0000\u0000€|\u0000\u0000pX\u0000\u0000 X\u0000\u0000ÐW\u0000\u0000€W\u0000\u00000W\u0000\u0000àV\u0000\u0000�V\u0000\u0000@V\u0000\u0000ðU\u0000\u0000 U\u0000\u0000PU\u0000\u0000ÀT\u0000\u00000T\u0000\u0000 S\u0000\u0000\u0010S\u0000\u0000€R\u0000\u0000ðQ\u0000\u0000`Q\u0000\u0000ÐP\u0000\u0000@P\u0000\u0000°O\u0000\u0000 N\u0000\u0000�M\u0000\u0000€L\u0000\u0000pK\u0000\u0000`J\u0000\u0000PI\u0000\u0000@H\u0000\u00000G\u0000\u0000 F\u0000\u0000\u0010E\u0000\u0000\u0000C\u0000\u0000ð@\u0000\u0000à>\u0000\u0000Ð<\u0000\u0000À:\u0000\u0000°8\u0000\u0000 6\u0000\u0000�4\u0000\u0000€2\u0000\u0000p0\u0000\u0000`.\u0000\u0000P,\u0000\u0000@*\u0000\u00000(\u0000\u0000 &\u0000\u0000\u0010$\u0000\u0000\u0000\"\u0000\u0000ð\u001F\u0000\u0000à\u001D\u0000\u0000Ð\u001B\u0000\u0000À\u0019\u0000\u0000°\u0017\u0000\u0000 \u0015\u0000\u0000�\u0013\u0000\u0000€\u0011\u0000\u0000d\u0011\u0000\u0000|\u0010\u0000\u0000l\u000F\u0000\u0000\\\u000E\u0000\u0000L"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curl: (3) URL rejected: Bad hostname\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 8177k  100 8177k    0     0  27.0M      0 --:--:-- --:--:-- --:--:-- 27.1M\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T15:20:14.976791Z",
     "start_time": "2024-04-23T15:20:14.966291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'ytick.left': False,\n",
    "    'xtick.labeltop': False,\n",
    "    'xtick.top': False,\n",
    "    'ytick.labelright': False,\n",
    "    'ytick.right': False\n",
    "})\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot, titlesize=16):\n",
    "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
    "    plt.subplot(*subplot)\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "\n",
    "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
    "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
    "    # Images and labels.\n",
    "    images = [image.numpy_view() for image in images]\n",
    "    gestures = [top_gesture for (top_gesture, _) in results]\n",
    "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
    "\n",
    "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images) // rows\n",
    "\n",
    "    # Size and spacing.\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols, 1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # Display gestures and hand landmarks.\n",
    "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
    "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
    "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "          hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "          ])\n",
    "\n",
    "          mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
    "\n",
    "    # Layout.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()"
   ],
   "id": "1d2d5c565392a67b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T15:20:19.137563Z",
     "start_time": "2024-04-23T15:20:18.640257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMAGE_FILENAMES = ['thumbs_down.jpg', 'victory.jpg', 'thumbs_up.jpg', 'pointing_up.jpg']\n",
    "\n",
    "for name in IMAGE_FILENAMES:\n",
    "  url = f'https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/{name}'\n",
    "  urllib.request.urlretrieve(url, name)\n"
   ],
   "id": "4930b9673ee336f7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T15:09:49.060996Z",
     "start_time": "2024-04-23T15:09:49.032993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "def resize_and_show(image):\n",
    "  h, w = image.shape[:2]\n",
    "  if h < w:\n",
    "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "  else:\n",
    "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "  cv2_imshow(img)\n",
    "\n",
    "\n",
    "# Preview the images.\n",
    "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
    "for name, image in images.items():\n",
    "  print(name)\n",
    "  resize_and_show(image)"
   ],
   "id": "e10f484fabeefd2e",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpatches\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cv2_imshow\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m      6\u001B[0m DESIRED_HEIGHT \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m480\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T15:27:38.798527Z",
     "start_time": "2024-04-23T15:27:38.764026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STEP 2: Create an GestureRecognizer object.\n",
    "base_options = mp.tasks.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "images = []\n",
    "results = []\n",
    "for image_file_name in IMAGE_FILENAMES:\n",
    "  # STEP 3: Load the input image.\n",
    "  image = mp.Image.create_from_file(image_file_name)\n",
    "\n",
    "  # STEP 4: Recognize gestures in the input image.\n",
    "  recognition_result = recognizer.recognize(image)\n",
    "\n",
    "  # STEP 5: Process the result. In this case, visualize it.\n",
    "  images.append(image)\n",
    "  top_gesture = recognition_result.gestures[0][0]\n",
    "  hand_landmarks = recognition_result.hand_landmarks\n",
    "  results.append((top_gesture, hand_landmarks))\n",
    "\n",
    "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
   ],
   "id": "3fe6e16a26af1396",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file at C:\\Users\\Nick Büttner\\PycharmProjects\\gesture_glide\\venv\\lib\\site-packages/C:\\Users\\Nick Büttner\\PycharmProjects\\gesture_glide\\gesture_recognizer.task, errno=22",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m base_options \u001B[38;5;241m=\u001B[39m mp\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mBaseOptions(model_asset_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgesture_recognizer.task\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m options \u001B[38;5;241m=\u001B[39m vision\u001B[38;5;241m.\u001B[39mGestureRecognizerOptions(base_options\u001B[38;5;241m=\u001B[39mbase_options)\n\u001B[1;32m----> 4\u001B[0m recognizer \u001B[38;5;241m=\u001B[39m \u001B[43mvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGestureRecognizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_from_options\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m images \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      7\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\PycharmProjects\\gesture_glide\\venv\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\gesture_recognizer.py:340\u001B[0m, in \u001B[0;36mGestureRecognizer.create_from_options\u001B[1;34m(cls, options)\u001B[0m\n\u001B[0;32m    317\u001B[0m   options\u001B[38;5;241m.\u001B[39mresult_callback(\n\u001B[0;32m    318\u001B[0m       gesture_recognizer_result,\n\u001B[0;32m    319\u001B[0m       image,\n\u001B[0;32m    320\u001B[0m       timestamp\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m _MICRO_SECONDS_PER_MILLISECOND,\n\u001B[0;32m    321\u001B[0m   )\n\u001B[0;32m    323\u001B[0m task_info \u001B[38;5;241m=\u001B[39m _TaskInfo(\n\u001B[0;32m    324\u001B[0m     task_graph\u001B[38;5;241m=\u001B[39m_TASK_GRAPH_NAME,\n\u001B[0;32m    325\u001B[0m     input_streams\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    338\u001B[0m     task_options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    339\u001B[0m )\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtask_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_graph_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable_flow_limiting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mode\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_RunningMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLIVE_STREAM\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpackets_callback\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult_callback\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gesture_glide\\venv\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:70\u001B[0m, in \u001B[0;36mBaseVisionTaskApi.__init__\u001B[1;34m(self, graph_config, running_mode, packet_callback)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m packet_callback:\n\u001B[0;32m     66\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     67\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe vision task is in image or video mode, a user-defined result \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     68\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcallback should not be provided.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     69\u001B[0m   )\n\u001B[1;32m---> 70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_runner \u001B[38;5;241m=\u001B[39m \u001B[43m_TaskRunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpacket_callback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_running_mode \u001B[38;5;241m=\u001B[39m running_mode\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Unable to open file at C:\\Users\\Nick Büttner\\PycharmProjects\\gesture_glide\\venv\\lib\\site-packages/C:\\Users\\Nick Büttner\\PycharmProjects\\gesture_glide\\gesture_recognizer.task, errno=22"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "c0a8ea737862a1f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aadaf35cdb5f1eab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
